{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Relationships Workflow\n",
    "\n",
    "#### 1. Split into chapters\n",
    "\n",
    "First, the book is split into chapters as the memory requirement for analysing the entire book at once is too large. \n",
    "\n",
    "In our case, the key delimiter to split is 6 consecutive newline characters `\\n` and the title of the chapter is taken as the first line of each block of text. The chapter splitting may not be perfect and some manual adjustments may be necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from chapter_splitter import split_chapters\n",
    "\n",
    "text_dir = 'text'\n",
    "book = 'worm'\n",
    "\n",
    "split_chapters(text_dir, book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Conduct Named Entity Recognition and Coreference using BookNLP \n",
    "\n",
    "The BookNLP library was used to conduct Named Entity Recognition and Coreference (matching characters and tracking when they are present even if pronouns are used).\n",
    "\n",
    "To install BookNLP, run the following on CLI.\n",
    "```bash\n",
    "pip install booknlp\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_coref import run_ner_coref\n",
    "\n",
    "ner_coref_data_dir = os.path.join('booknlp_output', book)\n",
    "text_dir = os.path.join('text', book)\n",
    "\n",
    "run_ner_coref(text_dir, ner_coref_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Searching and Matching Main Characters\n",
    "\n",
    "BookNLP provides a list of characters in each of the chapters. The next task is to match them across the chapters for the entire text.\n",
    "\n",
    "Firstly, BookNLP refers to tries to refer to the identified characters with proper nouns if possible, (followed by a common noun, then a pronoun). The nouns were matched across the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_main_char import get_main_char\n",
    "\n",
    "characters_data_dir = os.path.join('characters', book)\n",
    "\n",
    "get_main_char(ner_coref_data_dir, characters_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a bit of manual work is needed. Some characters have aliases or titles, and sometimes they are referred to with or without their surnames. The more common nouns were tracked and matched based on knowledge of the text (having read the book helps).\n",
    "\n",
    "The most common characters can be viewed from the `main_characters.json` file in the `characters_data_dir` directory. Adjust `TOP_N_CHAR` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "TOP_N_CHAR = 200\n",
    "\n",
    "\n",
    "with open(os.path.join(characters_data_dir, \"main_characters.json\"), 'r') as file:\n",
    "    main_characters_data = json.load(file)\n",
    "\n",
    "main_characters_data = dict(sorted(main_characters_data.items(), key = lambda x: x[1][\"count\"], reverse=True))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for name, det in main_characters_data.items():\n",
    "    if count == TOP_N_CHAR: \n",
    "        break\n",
    "\n",
    "    print(name)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_character_aliases.json` file will have to be created and stored in the `characters_data_dir`. An example  of the file is as follows:\n",
    "\n",
    "```json\n",
    "[\n",
    "    [\"Name of Person 0\", \"Alias 1 of Person 0\", \"Alias 2 of Person 0\"],\n",
    "    [\"Name of Person 1\", \"Alias 1 of Person 1\"],\n",
    "    [\"Name of Person 2\"]\n",
    "]\n",
    "```\n",
    "\n",
    "Each character can have any number of aliases, so long as it is more than 1. The strings in the list must be an exact match with the nouns in the `main_characters.json` list.\n",
    "\n",
    "Now, the main characters can be matched across the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consolidate_main_char import consolidate_main_char\n",
    "\n",
    "consolidate_main_char(characters_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `consolidated_id` in the newly generated `main_characters_consolidated.json` file will be the index of the character in the list manually created in `main_character_aliases.json`. That character's names and aliases are in the corresponding index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fetching Relevant Sentences\n",
    "\n",
    "The next step is to collate all sentences in the book which feature 2 or more characters, and link them to each character's `consolidated_id`.\n",
    "\n",
    "A `relevant_sentences.csv` file is generated under the directory of each chapter in the output directory of BookNLP (`ner_coref_data_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_relevant_sentences import get_relevant_sentences_in_book\n",
    "\n",
    "get_relevant_sentences_in_book(\n",
    "    ner_coref_data_dir,\n",
    "    text_dir,\n",
    "    characters_data_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
