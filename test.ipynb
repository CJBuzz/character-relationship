{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF\n",
    "\n",
    "Search for main characters throughout the whole book first, then do a reference reassignment to top 50-70 most common characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "book = 'worm'\n",
    "\n",
    "ner_coref_data_dir = os.path.join('output', book)\n",
    "characters_data_dir = os.path.join('temp_files', book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Lisa answered my question, ignoring her.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:19<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Sentiment Analysis! Computing averages...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob \n",
    "from tqdm import tqdm\n",
    "\n",
    "bookname = 'worm'\n",
    "\n",
    "ner_coref_data_dir = os.path.join('output', bookname)\n",
    "characters_data_dir = os.path.join('temp_files', bookname)\n",
    "\n",
    "\n",
    "def collate_relations(\n",
    "    ner_coref_data_dir: str,\n",
    "    characters_data_dir: str\n",
    ") -> None:\n",
    "    main_characters_aliases_file_path = os.path.join(characters_data_dir, 'main_characters_aliases.json')\n",
    "\n",
    "    if not os.path.exists(main_characters_aliases_file_path):\n",
    "        raise FileNotFoundError('Missing main_characters_aliases.json in given directory!')\n",
    "\n",
    "    with open(main_characters_aliases_file_path, 'r') as file: \n",
    "        main_char_list = json.load(file)\n",
    "\n",
    "    num_chars = len(main_char_list)\n",
    "    num_chapters = len(os.listdir(ner_coref_data_dir))\n",
    "\n",
    "    relations_arr = np.zeros((num_chars, num_chars, 2, num_chapters))\n",
    "    relations_arr_avg = np.zeros((num_chars, num_chars))\n",
    "    interactions_arr = np.zeros((num_chars, num_chars))\n",
    "\n",
    "    for chapter_num, chapter in tqdm(enumerate(os.listdir(ner_coref_data_dir)), total=num_chapters):\n",
    "        relevant_sentences_file_path = os.path.join(ner_coref_data_dir, chapter, 'relevant_sentences.csv')\n",
    "        df = pd.read_csv(relevant_sentences_file_path)\n",
    "\n",
    "        df['Sentiment'] = 0.0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            char_list = set(json.loads(row['characters']) + (json.loads(row['speaker']) if type(row['speaker']) is str else []))\n",
    "            if len(char_list) < 2:\n",
    "                continue\n",
    "\n",
    "            b = TextBlob(row['words'])\n",
    "            sentiment = b.sentiment[0]\n",
    "\n",
    "            df.loc[idx, 'Sentiment'] = sentiment\n",
    "\n",
    "            for char1 in char_list:\n",
    "                for char2 in char_list:\n",
    "                    if char1 == char2:\n",
    "                        continue\n",
    "\n",
    "                    relations_arr[char1][char2][0][chapter_num] += sentiment\n",
    "                    relations_arr[char1][char2][1][chapter_num] += 1\n",
    "\n",
    "        df.to_csv(relevant_sentences_file_path, index=False)\n",
    "\n",
    "    character_relations_file_path = os.path.join(characters_data_dir, 'character-relations.npy')\n",
    "    np.save(character_relations_file_path, relations_arr)\n",
    "\n",
    "    print(\"Completed Sentiment Analysis! Computing averages...\")\n",
    "\n",
    "    for char1 in range(relations_arr_avg.shape[0]):\n",
    "        for char2 in range(relations_arr_avg.shape[1]):\n",
    "            if char1 == char2: \n",
    "                continue\n",
    "\n",
    "            count = np.sum(relations_arr[char1][char2][1])\n",
    "\n",
    "            if not count: \n",
    "                continue\n",
    "\n",
    "            relations_arr_avg[char1][char2] = np.sum(relations_arr[char1][char2][0])/count\n",
    "            interactions_arr[char1][char2] = count\n",
    "\n",
    "    character_relations_avg_file_path = os.path.join(characters_data_dir, 'character-relations_avg.npy')\n",
    "    np.save(character_relations_avg_file_path, relations_arr_avg)\n",
    "\n",
    "    interactions_file_path = os.path.join(characters_data_dir, 'interactions.npy')\n",
    "    np.save(interactions_file_path, interactions_arr)\n",
    "\n",
    "    print(\"Completed!\")\n",
    "\n",
    "\n",
    "collate_relations(ner_coref_data_dir, characters_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "“I’ve killed before.\n"
     ]
    }
   ],
   "source": [
    "relevant_sentences_file_path = os.path.join(ner_coref_data_dir, 'Part-304-Interlude__End', 'relevant_sentences.csv')\n",
    "\n",
    "df = pd.read_csv(relevant_sentences_file_path)\n",
    "row = df.iloc[4]\n",
    "print(len(json.loads(row['characters'])))\n",
    "print(row['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = os.path.join(characters_data_dir, 'interactions.npy')\n",
    "\n",
    "interactions_arr = np.load(interactions_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['NARRATOR', 'Taylor', 'Taylor Hebert', 'Ms. Hebert', 'Skitter', 'Weaver'] 358.0\n",
      "1 ['Tattletale', 'Lisa'] 94.0\n",
      "2 ['Grue', 'Brian'] 20.0\n",
      "3 ['Bitch', 'Rachel', 'Rachel Lindt'] 9.0\n",
      "4 ['Krouse', 'Francis', 'Trickster'] 5.0\n",
      "5 ['Coil', 'Thomas Calvert', 'Thomas', 'Calvert', 'Director Calvert', 'Commander Calvert'] 1.0\n",
      "6 ['Lung', 'Kenta'] 19.0\n",
      "7 ['Noelle', 'Echidna'] 0.0\n",
      "8 ['Imp', 'Aisha'] 5.0\n",
      "9 ['Regent', 'Alec'] 1.0\n",
      "10 ['Jack', 'Jack Slash'] 45.0\n",
      "11 ['Miss Militia', 'Hannah', 'Hana'] 3.0\n",
      "12 ['Scion', 'the Warrior', 'The golden man', 'the golden man'] 2.0\n",
      "13 ['Chevalier'] 1.0\n",
      "14 ['Bonesaw', 'Riley'] 85.0\n",
      "15 ['Defiant', 'Armsmaster', 'Collin', 'Colin'] 5.0\n",
      "16 ['Amy', 'Amy Dallon', 'Panacea', 'Amelia', 'Ames'] 0.0\n",
      "17 ['Golem', 'Theo', 'Theodore Anders', 'Theodore'] 0.0\n",
      "18 ['Weld'] 0.0\n",
      "19 ['Shadow Stalker', 'Sophia'] 3.0\n",
      "20 ['Dinah', 'Dinah Alcott'] 0.0\n",
      "21 ['my dad', 'Dad', 'dad', 'Danny', 'Mr. Hebert'] 0.0\n",
      "22 ['Emma', 'Emma Barnes'] 0.0\n",
      "23 ['Leviathan'] 1.0\n",
      "24 ['Eidolon'] 0.0\n",
      "25 ['Dragon'] 4.0\n",
      "26 ['Alexandria', 'Rebecca'] 0.0\n",
      "27 ['Legend', 'Keith'] 1.0\n",
      "28 ['Parian', 'Sabah'] 2.0\n",
      "29 ['Tecton'] 0.0\n",
      "30 ['Vista', 'A small figure who could only be Vista'] 0.0\n",
      "31 ['Mannequin'] 7.0\n",
      "32 ['Marquis'] 86.0\n",
      "33 ['Kid Win'] 0.0\n",
      "34 ['Clockblocker', 'Clocksie', 'Dennis'] 3.0\n",
      "35 ['Cody', 'Perdition'] 0.0\n",
      "36 ['Teacher'] 4.0\n",
      "37 ['Flechette', 'Foil', 'Lily'] 6.0\n",
      "38 ['Siberian', 'Manton', 'William Manton'] 44.0\n",
      "39 ['Charlotte', 'Charlottte'] 0.0\n",
      "40 ['Hookwolf'] 1.0\n",
      "41 ['Bakuda'] 1.0\n",
      "42 ['Accord'] 0.0\n",
      "43 ['Tagg', 'Director Tagg', 'Director James Tagg'] 0.0\n",
      "44 ['Saint'] 0.0\n",
      "45 ['Behemoth'] 0.0\n",
      "46 ['Sierra'] 0.0\n",
      "47 ['Sundancer', 'Marissa'] 2.0\n",
      "48 ['Shatterbird'] 1.0\n",
      "49 ['Cherish', 'Cherie', 'Cherie Vasil', 'A teenager with a red streak dyed into her dark hair'] 2.0\n",
      "50 ['Faultline'] 0.0\n",
      "51 ['Canary', 'Paige'] 5.0\n",
      "52 ['Victoria', 'Glory Girl'] 144.0\n",
      "53 ['Glenn'] 0.0\n",
      "54 ['Cuff'] 1.0\n",
      "55 ['Ballistic', 'Luke'] 1.0\n",
      "56 ['Kayden', 'Purity', 'Kayden Anders'] 0.0\n",
      "57 ['Genesis', 'Jess'] 0.0\n",
      "58 ['the Simurgh', 'The Simurgh', 'Simurgh', 'the winged Endbringer', 'The winged Endbringer'] 0.0\n",
      "59 ['Triumph'] 0.0\n",
      "60 ['Crawler'] 1.0\n",
      "61 ['Jamie', 'Battery'] 0.0\n",
      "62 ['Jessica', 'Jessica Yamada', 'Mrs. Yamada', 'Ms. Yamada', 'the therapist'] 5.0\n",
      "63 ['Grace'] 0.0\n",
      "64 ['Gregor', 'Gregor the Snail'] 0.0\n",
      "65 ['Burnscar', 'Mimi'] 0.0\n",
      "66 ['Rey', 'Blasto'] 0.0\n",
      "67 ['Glaistig Uaine', 'The Faerie Queen', 'Valkyrie', 'Ciara'] 2.0\n",
      "68 ['Sveta'] 0.0\n",
      "69 ['Newter'] 0.0\n",
      "70 ['the Number Man', 'The Number Man', 'Number Man', 'Kurt'] 0.0\n",
      "71 ['Satyr', 'Satyrical'] 0.0\n",
      "72 ['Aegis'] 1.0\n",
      "73 ['Ingenue'] 0.0\n",
      "74 ['Nilbog', 'the Goblin King'] 0.0\n",
      "75 ['Elle', 'Labyrinth'] 0.0\n",
      "76 ['Nero'] 0.0\n",
      "77 ['Kevin', 'Kevin Norton'] 0.0\n",
      "78 ['Contessa', 'Fortuna', 'The woman in the suit', 'A woman , tall , in a suit , carrying nothing with her'] 0.0\n",
      "79 ['Kaiser', 'Max', 'Max Anders'] 0.0\n",
      "80 ['Justin', 'Crusader'] 0.0\n",
      "81 ['Victor'] 0.0\n",
      "82 ['Skidmark'] 0.0\n",
      "83 ['Leet'] 0.0\n",
      "84 ['Gallant'] 22.0\n",
      "85 ['Phir Sē'] 0.0\n",
      "86 ['Butcher'] 0.0\n",
      "87 ['Wanton'] 0.0\n",
      "88 ['Carol', 'Brandish'] 45.0\n",
      "89 ['Oliver'] 0.0\n",
      "90 ['Prism'] 0.0\n",
      "91 ['Myrddin'] 0.0\n",
      "92 ['Citrine'] 0.0\n",
      "93 ['Night'] 0.0\n",
      "94 ['Valefor'] 0.0\n",
      "95 ['Revel'] 0.0\n",
      "96 ['Cricket'] 0.0\n",
      "97 ['Rime'] 0.0\n",
      "98 ['Brooks'] 0.0\n",
      "99 ['Chariot'] 0.0\n",
      "100 ['Forrest'] 0.0\n",
      "101 ['Mark', 'Flashbang'] 49.0\n",
      "102 ['Biter'] 0.0\n",
      "103 ['Assault', 'Madcap'] 0.0\n",
      "104 ['Mr. Gladly'] 0.0\n",
      "105 ['Emily', 'Piggot'] 0.0\n",
      "106 ['Topsy'] 0.0\n",
      "107 ['Pandora'] 0.0\n",
      "108 ['Oni Lee'] 2.0\n",
      "109 ['Gray Boy'] 0.0\n"
     ]
    }
   ],
   "source": [
    "main_characters_aliases_file_path = os.path.join(characters_data_dir, 'main_characters_aliases.json')\n",
    "\n",
    "with open(main_characters_aliases_file_path, 'r') as file: \n",
    "    main_char_list = json.load(file)\n",
    "\n",
    "for i in range(len(main_char_list)):\n",
    "    print(i, main_char_list[i], interactions_arr[16][i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
